package tifmo

import stree.Align
import stree.PI
import stree.Term
import stree.WORD
import stree.IR
import stree.PO
import resource.EnNgramDist
import mylib.oneFromEach
import knowledge.SemRole.SemRole
import knowledge.SemRole

import scala.collection.mutable
import scala.util.Sorting
import scala.actors._

package knowledge {
	
	class EnConfiFunc(alltw: Set[EnWord]) extends (Align => (Double, String)) {
		
		private[this] case class SetDist(k: List[Set[String]], v: mutable.Map[String, Long])
		private[this] case class SetNorm(k: List[Set[String]], v: Double)
		private[this] case class SetDot(k: Set[List[Set[String]]], v: Double)
		
		private[this] case class GetDot(a: Actor, k: Set[List[Set[String]]])
		private[this] case class GetNorm(a: Actor, k: List[Set[String]])
		private[this] case class GetDist(a: Actor, k: List[Set[String]])
		
		private[this] case class DotRet(k: Set[List[Set[String]]], v: Option[Double])
		private[this] case class NormRet(k: List[Set[String]], v: Option[Double])
		private[this] case class DistRet(k: List[Set[String]], v: Option[mutable.Map[String, Long]])
		
		private[this] object Caching extends Actor {
			
			private[this] val dist = mutable.Map.empty[List[Set[String]], mutable.Map[String, Long]]
			private[this] val norm = mutable.Map.empty[List[Set[String]], Double]
			private[this] val dot = mutable.Map.empty[Set[List[Set[String]]], Double]
			
			def act() {
				react {
					case SetDist(k, v) => {
						if (!dist.contains(k)) dist(k) = v
						act()
					}
					case SetNorm(k, v) => {
						if (!norm.contains(k)) norm(k) = v
						act()
					}
					case SetDot(k, v) => {
						if (!dot.contains(k)) dot(k) = v
						act()
					}
					case GetDot(a, k) => {
						a ! DotRet(k, dot.get(k))
						act()
					}
					case GetNorm(a, k) => {
						a ! NormRet(k, norm.get(k))
						act()
					}
					case GetDist(a, k) => {
						a ! DistRet(k, dist.get(k))
						act()
					}
				}
			}
		}
		Caching.start()
		
		private[this] case class GetNgram(a: Actor, k: List[Set[String]])
		private[this] case class NgramRet(k: List[Set[String]], v: mutable.Map[String, Long])
		
		private[this] object NgramReader extends Actor {
			
			def act() {
				react {
					case GetNgram(a, k) => {
						Caching ! GetDist(Actor.self, k)
						val ret = Actor.self.receive {
							case DistRet(kk, v) if kk == k => v
						} match {
							case Some(dist) => dist
							case None => {
								val tmp = mutable.Map.empty[String, Long]
								for (l <- oneFromEach[String](k)) {
									
									val sstr = l.mkString("", " ", "")
									System.err.println("ngram search start: " + sstr)
									
									EnNgramDist.get(sstr, tmp)
								}
								Caching ! SetDist(k, tmp)
								tmp
							}
						}
						a ! NgramRet(k, ret)
						act()
					}
				}
			}
		}
		NgramReader.start()
		
		private[this] def cossim(aws: Set[EnWord], bws: Set[EnWord], avoidws: Set[EnWord]) = {
			
			def surflex(x: EnWord) = {
				if (x.mypos == "D") {
					Set(x.surf)
				} else {
					val tmp = if (x.ner == "O") x.surf.toLowerCase else x.surf
					Set(tmp, x.lem)
				}
			}
			
			def genlist(ws: Set[EnWord]) = {
				val wl = ws.toList
				val wlm = wl.map(surflex(_))
				Sorting.stableSort[Set[String], (String, String)](wlm, x => (x.max, x.min)).toList
			}
			
			val awlst = genlist(aws)
			val bwlst = genlist(bws)
			
			def lookup(sss: List[Set[String]]) = {
				NgramReader ! GetNgram(Actor.self, sss)
				val dist = Actor.self.receive {
					case NgramRet(kk, v) if kk == sss => v
				}
				Caching ! GetNorm(Actor.self, sss)
				val norm = Actor.self.receive {
					case NormRet(kk, v) if kk == sss => v
				} match {
					case Some(x) => x
					case None => {
						//val sqs = dist.keys.par.map(k => (dist(k) * dist(k)).toDouble)
						//val ret = math.sqrt(sqs.fold(0.0)(_ + _))
						val sqs = dist.keys.map(k => (dist(k) * dist(k)).toDouble)
						val ret = math.sqrt((0.0 /: sqs)(_ + _))
						Caching ! SetNorm(sss, ret)
						ret
					}
				}
				(dist, norm)
			}
			
			val (adist, anorm) = lookup(awlst)
			val (bdist, bnorm) = lookup(bwlst)
			
			val dot = {
				val k = Set(awlst, bwlst)
				Caching ! GetDot(Actor.self, k)
				Actor.self.receive {
					case DotRet(kk, v) if kk == k => v
				} match {
					case Some(x) => x
					case None => {
						val ret = if (adist.size < bdist.size) {
							adist.keys.par.filter(k => bdist.contains(k)).map(k => {
								(adist(k) * bdist(k)).toDouble
							}).fold(0.0)(_ + _)
						} else {
							bdist.keys.par.filter(k => adist.contains(k)).map(k => {
								(adist(k) * bdist(k)).toDouble
							}).fold(0.0)(_ + _)
						}
						Caching ! SetDot(k, ret)
						ret
					}
				}
			}
			
			val avoid = avoidws.flatMap(surflex(_))
			
			val sustdot = (dot /: avoid)((x, k) => if (adist.contains(k) && bdist.contains(k)) x - adist(k) * bdist(k) else x)
			
			def substract(x: mutable.Map[String, Long], normx: Double) = {
				val pre = (normx /: avoid)((a, b) => if (x.contains(b)) a - x(b) * x(b) else a)
				if (pre.abs < 0.1) 1.0 else pre
			}
			
			sustdot / substract(adist, anorm) / substract(bdist, bnorm)
		}
		
		def apply(algn: Align) = {
	
			val cws = algn.clue.src.map(_.term.word.asInstanceOf[EnWord]).toSet
			/*
			val (contra, limcontra) = {
				
				var ret = false
				var ret2 = false
				
				def getRWR(x: PI) = {
					
					var r1lst = Nil:List[SemRole]
					var wlst = Nil:List[EnWord]
					var r2lst = Nil:List[SemRole]
					
					def recur(y: Term) {
						y match {
							case WORD(word) => {
								wlst = word.asInstanceOf[EnWord] :: wlst
								r2lst = null :: r2lst
							}
							case IR(core, sats) => {
								if (sats.isEmpty) {
									recur(core)
								} else {
									assert(sats.size == 1)
									val (r, itm) = sats.head
									wlst = core.word.asInstanceOf[EnWord] :: wlst
									r2lst = r :: r2lst
									
									r1lst = itm.intoR :: r1lst
									recur(itm.term)
									
								}
							}
							case PO(tm, r) => {
								recur(tm)
							}
							case _ => throw new Exception("???!")
						}
					}
					
					r1lst = x.intoR :: r1lst
					recur(x.term)
					
					(wlst zip (r1lst zip r2lst)).reverse
				}
				
				val trwr = getRWR(algn.tp.acc)
				val hrwr = {
					val pre = getRWR(algn.hp.acc)
					if (cws.exists(z => z.synonymTo(trwr.head._1) || z.antonymTo(trwr.head._1))) {
						pre
					} else {
						pre.dropWhile(y => cws.exists(z => z.synonymTo(y._1) || z.antonymTo(y._1)))
					}
				}
				
				for ((w, (r1, r2)) <- hrwr; if w.hasPolarity) {
					if (trwr.forall(x => !w.samePolarity(x._1))) {
						ret = true
					}
				}
				
				var unmatchsign1 = false
				for ((x, (r1, r2)) <- hrwr; if !x.sign) {
					if (trwr.exists(y => x.synonymTo(y._1, true) && !x.synonymTo(y._1) && y._1.sign)) unmatchsign1 = true
				}
				var unmatchsign2 = false
				for ((x, (r1, r2)) <- trwr; if !x.sign) {
					if (hrwr.exists(y => x.synonymTo(y._1, true) && !x.synonymTo(y._1) && y._1.sign)) unmatchsign2 = true
				}
				if ((unmatchsign1 ^ unmatchsign2) ^ {
					val tneg = trwr.filter(_._1.isNegWord)
					val hneg = hrwr.filter(_._1.isNegWord)
					if (tneg.isEmpty ^ hneg.isEmpty) {
						!trwr.exists(x => hneg.exists(y => x._1.synonymTo(y._1))) && 
							!hrwr.exists(x => tneg.exists(y => x._1.synonymTo(y._1)))
					} else {
						false
					}
					//(trwr.exists(_._1.isNegWord) ^ hrwr.exists(_._1.isNegWord))
				}) {
					ret = true
					//ret2 = unmatchsign1 ^ unmatchsign2
				}
				
				
				val sensitiveRoles = Set(SemRole.SBJ, SemRole.OBJ, SemRole.IOBJ)
				for {
					(tw, (tr1, tr2)) <- trwr
					(hw, (hr1, hr2)) <- hrwr
					if (tw.mypos == "V" && hw.mypos == "V" && tw.synonymTo(hw))
				} {
					if ((sensitiveRoles.contains(tr1) && sensitiveRoles.contains(hr1) && tr1 != hr1) || (sensitiveRoles.contains(tr2) && sensitiveRoles.contains(hr2) && tr2 != hr2)) {
						ret = true
						ret2 = true
					}
				}
				
				(ret, ret2)
			}
			*/
			val (contra, limcontra) = (false, false)
			
			def trimHead(x: List[PI]) = {
				val wl = x.map(_.term.word.asInstanceOf[EnWord])
				val dwl = wl.dropWhile(y => cws.exists(z => y.synonymTo(z) || y.antonymTo(z)))
				dwl.toSet
			}
			
			val (tws, hws) = if (algn.soft) {
				(trimHead(algn.tp.src.init).filter(y => !y.isStopwd || y.mypos == "D"), trimHead(algn.hp.src.init).filter(y => !y.isStopwd || y.mypos == "D"))
			} else {
				(trimHead(algn.tp.src), trimHead(algn.hp.src))
			}
			
			if (!hws.filter(x => x.mypos == "D" && !tws.exists(x.synonymTo(_, true))).isEmpty) {
				
				(0.0, "0-FILTERED")
				
			} else if ((tws.isEmpty && algn.soft) || hws.isEmpty) {
				
				val eval = (cws.size + hws.filter(x => cws.exists(x.synonymTo(_, true))).size + (tws.filter(_.mypos == "V").size max 1) - 1).toDouble
				(0.1 + 0.15 / eval, "c-RELATION")
				
			} else if (hws.forall(x => tws.exists(x.synonymTo(_, true)))) {
				
				if (contra) {
					if (algn.soft) {
						(0.15, "a-CONTRA")
					} else {
						(0.10001, "a-CONTRA")
					}
				} else {
					val eval = (tws.filter(_.mypos == "V").size max 1).toDouble
					(0.1 + 0.2 / eval, "d-SUPER")
				}
				
			} else if (tws.isEmpty || hws.forall(x => !tws.exists(x.synonymTo(_, true)) && alltw.exists(x.synonymTo(_, true)))) {
				
				(0.0, "0-FILTERED")
				
			} else if (tws.forall(x => hws.exists(x.synonymTo(_, true)))) {
				
				if (contra) {
					if (algn.soft) {
						(0.15, "a-CONTRA")
					} else {
						(0.10001, "a-CONTRA")
					}
				} else {
					val eval = ((hws.size - tws.size) max 1).toDouble
					(0.1 + 0.1 / eval, "b-SUB")
				}
				
			} else if (hws.size == 1 && tws.forall(x => hws.head.getContext(3, 4).exists(x.synonymTo(_, true)))) {
				
				(0.0, "0-FILTERED")
				
			} else if (tws.size <= 3 && hws.size <= 3) {
				
				val eval = (1.0 - math.log10(cossim(tws, hws, cws)))
				if (limcontra) {
					if (eval < 10.0) {
						if (algn.soft) {
							(1.0 / eval, "a-CONTRA")
						} else {
							(0.10001, "a-CONTRA")
						}
					} else {
						(0.0, "0-FILTERED")
					}
				} else {
					(1.0 / eval, "e-SIM")
				}
				
			} else {
				
				(0.0, "0-FILTERED")
				
			}
		}
	}
}
