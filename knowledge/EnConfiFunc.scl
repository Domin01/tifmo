package tifmo

import stree.Align
import stree.PI
import resource.EnNgramDist
import mylib.oneFromEach

import scala.collection.mutable
import scala.util.Sorting
import scala.actors._

package knowledge {
	
	class EnConfiFunc extends (Align => Double) {
		
		private[this] case class SetDist(k: List[Set[String]], v: mutable.Map[String, Long])
		private[this] case class SetNorm(k: List[Set[String]], v: Double)
		private[this] case class SetDot(k: Set[List[Set[String]]], v: Double)
		
		private[this] case class GetDot(a: Actor, k: Set[List[Set[String]]])
		private[this] case class GetNorm(a: Actor, k: List[Set[String]])
		private[this] case class GetDist(a: Actor, k: List[Set[String]])
		
		private[this] case class DotRet(k: Set[List[Set[String]]], v: Option[Double])
		private[this] case class NormRet(k: List[Set[String]], v: Option[Double])
		private[this] case class DistRet(k: List[Set[String]], v: Option[mutable.Map[String, Long]])
		
		private[this] object Caching extends Actor {
			
			private[this] val dist = mutable.Map.empty[List[Set[String]], mutable.Map[String, Long]]
			private[this] val norm = mutable.Map.empty[List[Set[String]], Double]
			private[this] val dot = mutable.Map.empty[Set[List[Set[String]]], Double]
			
			def act() {
				react {
					case SetDist(k, v) => {
						if (!dist.contains(k)) dist(k) = v
						act()
					}
					case SetNorm(k, v) => {
						if (!norm.contains(k)) norm(k) = v
						act()
					}
					case SetDot(k, v) => {
						if (!dot.contains(k)) dot(k) = v
						act()
					}
					case GetDot(a, k) => {
						a ! DotRet(k, dot.get(k))
						act()
					}
					case GetNorm(a, k) => {
						a ! NormRet(k, norm.get(k))
						act()
					}
					case GetDist(a, k) => {
						a ! DistRet(k, dist.get(k))
						act()
					}
				}
			}
		}
		Caching.start()
		
		private[this] case class GetNgram(a: Actor, k: List[Set[String]])
		private[this] case class NgramRet(k: List[Set[String]], v: mutable.Map[String, Long])
		
		private[this] object NgramReader extends Actor {
			
			def act() {
				react {
					case GetNgram(a, k) => {
						Caching ! GetDist(Actor.self, k)
						val ret = Actor.self.receive {
							case DistRet(kk, v) if kk == k => v
						} match {
							case Some(dist) => dist
							case None => {
								val tmp = mutable.Map.empty[String, Long]
								for (l <- oneFromEach[String](k)) {
									
									val sstr = l.mkString("", " ", "")
									System.err.println("ngram search start: " + sstr)
									
									EnNgramDist.get(sstr, tmp)
								}
								Caching ! SetDist(k, tmp)
								tmp
							}
						}
						a ! NgramRet(k, ret)
						act()
					}
				}
			}
		}
		NgramReader.start()
		
		private[this] def cossim(aws: Set[EnWord], bws: Set[EnWord], avoidws: Set[EnWord]) = {
			
			def surflex(x: EnWord) = {
				if (x.mypos == "D") {
					Set(x.surf)
				} else {
					val tmp = if (x.ner == "O") x.surf.toLowerCase else x.surf
					Set(tmp, x.lem)
				}
			}
			
			def genlist(ws: Set[EnWord]) = {
				val wl = ws.toList
				val wlm = wl.map(surflex(_))
				Sorting.stableSort[Set[String], (String, String)](wlm, x => (x.max, x.min)).toList
			}
			
			val awlst = genlist(aws)
			val bwlst = genlist(bws)
			
			def lookup(sss: List[Set[String]]) = {
				NgramReader ! GetNgram(Actor.self, sss)
				val dist = Actor.self.receive {
					case NgramRet(kk, v) if kk == sss => v
				}
				Caching ! GetNorm(Actor.self, sss)
				val norm = Actor.self.receive {
					case NormRet(kk, v) if kk == sss => v
				} match {
					case Some(x) => x
					case None => {
						val sqs = dist.keys.par.map(k => (dist(k) * dist(k)).toDouble)
						val ret = math.sqrt(sqs.fold(0.0)(_ + _))
						Caching ! SetNorm(sss, ret)
						ret
					}
				}
				(dist, norm)
			}
			
			val (adist, anorm) = lookup(awlst)
			val (bdist, bnorm) = lookup(bwlst)
			
			val dot = {
				val k = Set(awlst, bwlst)
				Caching ! GetDot(Actor.self, k)
				Actor.self.receive {
					case DotRet(kk, v) if kk == k => v
				} match {
					case Some(x) => x
					case None => {
						val ret = if (adist.size < bdist.size) {
							adist.keys.par.filter(k => bdist.contains(k)).map(k => {
								(adist(k) * bdist(k)).toDouble
							}).fold(0.0)(_ + _)
						} else {
							bdist.keys.par.filter(k => adist.contains(k)).map(k => {
								(adist(k) * bdist(k)).toDouble
							}).fold(0.0)(_ + _)
						}
						Caching ! SetDot(k, ret)
						ret
					}
				}
			}
			
			val avoid = avoidws.flatMap(surflex(_))
			
			val sustdot = (dot /: avoid)((x, k) => if (adist.contains(k) && bdist.contains(k)) x - adist(k) * bdist(k) else x)
			
			def substract(x: mutable.Map[String, Long], normx: Double) = {
				val pre = (normx /: avoid)((a, b) => if (x.contains(b)) a - x(b) * x(b) else a)
				if (pre.abs < 0.1) 1.0 else pre
			}
			
			sustdot / substract(adist, anorm) / substract(bdist, bnorm)
		}
		
		def apply(algn: Align) = {
	
			val cws = algn.clue.src.map(_.term.word.asInstanceOf[EnWord]).toSet
			
			def trimHead(x: List[PI]) = {
				val wl = x.map(_.term.word.asInstanceOf[EnWord])
				val dwl = wl.dropWhile(y => cws.exists(y.synonymTo(_)))
				dwl.filter(!_.isStopwd).toSet
			}
			
			val (tws, hws) = if (algn.soft) {
				val tmp = trimHead(algn.tp.src.init)
				if (tmp.isEmpty) {
					(trimHead(algn.tp.src), trimHead(algn.hp.src))
				} else {
					val tlw = algn.tp.src.last.term.word.asInstanceOf[EnWord]
					if (tlw.ner != "O" || tlw.synonymTo(algn.hp.src.last.term.word.asInstanceOf[EnWord], true)) {
						(tmp, trimHead(algn.hp.src.init))
					} else {
						(trimHead(algn.tp.src), trimHead(algn.hp.src.init))
					}
				}
			} else {
				(trimHead(algn.tp.src), trimHead(algn.hp.src))
			}
			
			if ((tws.isEmpty && algn.soft) || hws.isEmpty) {
				
				0.3 + 0.4 / (cws.size + hws.filter(x => cws.exists(x.synonymTo(_, true))).size)
				
			} else if (hws.forall(x => tws.exists(x.synonymTo(_, true)))) {
				
				0.8
				
			} else if (tws.isEmpty) {
				
				0.0
				
			} else if (tws.forall(x => hws.exists(x.synonymTo(_, true)))) {
				
				0.1 + 1.0 / (3 + hws.size - tws.size)
				
			} else if (hws.size == 1 && tws.forall(x => hws.head.getContext(3, 4).exists(x.synonymTo(_, true)))) {
				
				0.0
				
			} else if (tws.size <= 3 && hws.size <= 3) {
				
				1.0 / (1.0 - math.log10(cossim(tws, hws, cws)))
				
			} else {
				
				0.0
				
			}
		}
	}
}
