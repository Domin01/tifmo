package tifmo

import stree.Align
import stree.PI
import resource.EnNgramDist
import mylib.oneFromEach

import scala.collection.mutable
import scala.util.Sorting
import scala.actors._

package knowledge {
	
	class EnConfiFunc extends (Align => Double) {
		
		private[this] case class DistRet(k: List[Set[String]], v: Option[(mutable.Map[String, Long], Double)])
		private[this] object DistCache extends Actor {
			
			private[this] val cache = mutable.Map.empty[List[Set[String]], (mutable.Map[String, Long], Double)]
			
			def act() {
				react {
					case (k:List[Set[String]], (dist:mutable.Map[String, Long], norm:Double)) => {
						if (!cache.contains(k)) {
							cache(k) = (dist, norm)
						}
						act()
					}
					case (actor:Actor, k:List[Set[String]]) => {
						actor ! (if (cache.contains(k)) {
							DistRet(k, Some(cache(k)))
						} else {
							DistRet(k, None)
						})
						act()
					}
				}
			}
		}
		DistCache.start()
		private[this] case class NgramRet(k: List[Set[String]], v: Any)
		private[this] object NgramReader extends Actor {
			
			def act() {
				react {
					case (actor: Actor, k:List[Set[String]]) => {
						DistCache ! (Actor.self, k)
						val ret = Actor.self.receive {
							case DistRet(kk, v) if kk == k => v match {
								case Some((dist:mutable.Map[String, Long], norm:Double)) => (dist:mutable.Map[String, Long], norm:Double)
								case None => {
									val tmp = mutable.Map.empty[String, Long]
									for (l <- oneFromEach[String](k)) {
										
										val sstr = l.mkString("", " ", "")
										System.err.println("ngram search start: " + sstr)
										
										EnNgramDist.get(sstr, tmp)
									}
									tmp
								}
							}
						}
						actor ! NgramRet(k, ret)
						act()
					}
				}
			}
		}
		NgramReader.start()
		private[this] case class DotRet(k: Set[List[Set[String]]], v: Option[Double])
		private[this] object DotCache extends Actor {
			
			private[this] val cache = mutable.Map.empty[Set[List[Set[String]]], Double]
			
			def act() {
				react {
					case (k:Set[List[Set[String]]], v:Double) => {
						if (!cache.contains(k)) {
							cache(k) = v
						}
						act()
					}
					case (actor:Actor, k:Set[List[Set[String]]]) => {
						actor ! (if (cache.contains(k)) {
							DotRet(k, Some(cache(k)))
						} else {
							DotRet(k, None)
						})
						act()
					}
				}
			}
		}
		DotCache.start()
		private[this] def cossim(aws: Set[EnWord], bws: Set[EnWord], avoidws: Set[EnWord]) = {
			
			def surflex(x: EnWord) = {
				if (x.mypos == "D") {
					Set(x.surf)
				} else {
					val tmp = if (x.ner == "O") x.surf.toLowerCase else x.surf
					Set(tmp, x.lem)
				}
			}
			
			def genlist(ws: Set[EnWord]) = {
				val wl = ws.toList
				val wlm = wl.map(surflex(_))
				Sorting.stableSort[Set[String], (String, String)](wlm, x => (x.max, x.min)).toList
			}
			
			val awlst = genlist(aws)
			val bwlst = genlist(bws)
			
			def lookup(sss: List[Set[String]]) = {
				DistCache ! (Actor.self, sss)
				Actor.self.receive {
					case DistRet(kk, v) if kk == sss => v match {
						case Some((dist:mutable.Map[String, Long], norm:Double)) => (dist, norm)
						case None => {
							NgramReader ! (Actor.self, sss)
							Actor.self.receive {
								case NgramRet(kk, v) if kk == sss => v match {
									case (dist:mutable.Map[String, Long], norm:Double) => {
										(dist, norm)
									}
									case dist:mutable.Map[String, Long] => {
										val sqs = dist.keys.par.map(k => (dist(k) * dist(k)).toDouble)
										val norm = math.sqrt(sqs.fold(0.0)(_ + _))
										val ret = (dist, norm)
										DistCache ! (sss, ret)
										ret
									}
								}
							}
						}
					}
				}
			}
			
			val (adist, anorm) = lookup(awlst)
			val (bdist, bnorm) = lookup(bwlst)
			
			val dot = {
				val k = Set(awlst, bwlst)
				DotCache ! (Actor.self, k)
				Actor.self.receive {
					case DotRet(kk, v) if kk == k => v match {
						case Some(x:Double) => x
						case None => {
							val ret = if (adist.size < bdist.size) {
								adist.keys.par.filter(k => bdist.contains(k)).map(k => {
									(adist(k) * bdist(k)).toDouble
								}).fold(0.0)(_ + _)
							} else {
								bdist.keys.par.filter(k => adist.contains(k)).map(k => {
									(adist(k) * bdist(k)).toDouble
								}).fold(0.0)(_ + _)
							}
							DotCache ! (k, ret)
							ret
						}
					}
				}
			}
			
			val avoid = avoidws.flatMap(surflex(_))
			
			val sustdot = (dot /: avoid)((x, k) => if (adist.contains(k) && bdist.contains(k)) x - adist(k) * bdist(k) else x)
			
			def substract(x: mutable.Map[String, Long], normx: Double) = {
				val pre = (normx /: avoid)((a, b) => if (x.contains(b)) a - x(b) * x(b) else a)
				if (pre.abs < 0.1) 1.0 else pre
			}
			
			sustdot / substract(adist, anorm) / substract(bdist, bnorm)
		}
		
		def apply(algn: Align) = {
	
			val cws = algn.clue.src.map(_.term.word.asInstanceOf[EnWord]).toSet
			
			def trimHead(x: List[PI]) = {
				val wl = x.map(_.term.word.asInstanceOf[EnWord])
				val dwl = wl.dropWhile(y => cws.exists(y.synonymTo(_)))
				dwl.filter(!_.isStopwd).toSet
			}
			
			val (tws, hws) = if (algn.soft) {
				val tmp = trimHead(algn.tp.src.init)
				if (tmp.isEmpty) {
					(trimHead(algn.tp.src), trimHead(algn.hp.src))
				} else {
					val tlw = algn.tp.src.last.term.word.asInstanceOf[EnWord]
					if (tlw.ner != "O" || tlw.synonymTo(algn.hp.src.last.term.word.asInstanceOf[EnWord], true)) {
						(tmp, trimHead(algn.hp.src.init))
					} else {
						(trimHead(algn.tp.src), trimHead(algn.hp.src.init))
					}
				}
			} else {
				(trimHead(algn.tp.src), trimHead(algn.hp.src))
			}
			
			if ((tws.isEmpty && algn.soft) || hws.isEmpty) {
				
				0.3 + 0.4 / (cws.size + hws.filter(x => cws.exists(x.synonymTo(_, true))).size)
				
			} else if (hws.forall(x => tws.exists(x.synonymTo(_, true)))) {
				
				0.8
				
			} else if (tws.isEmpty) {
				
				0.0
				
			} else if (tws.forall(x => hws.exists(x.synonymTo(_, true)))) {
				
				0.1 + 1.0 / (3 + hws.size - tws.size)
				
			} else if (hws.size == 1 && tws.forall(x => hws.head.getContext(2, 3).exists(x.synonymTo(_, true)))) {
				
				0.0
				
			} else if (tws.size <= 3 && hws.size <= 3) {
				
				1.0 / (1.0 - math.log10(cossim(tws, hws, cws)))
				
			} else {
				
				0.0
				
			}
		}
	}
}
